{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist on Disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 424 0 (offset 0)\n",
      "Ignoring wrong pointing object 580 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 424 0 (offset 0)\n",
      "Ignoring wrong pointing object 580 0 (offset 0)\n",
      "Parsing nodes: 100%|██████████| 104/104 [00:00<00:00, 1533.41it/s]\n",
      "Generating embeddings: 100%|██████████| 103/103 [00:02<00:00, 45.67it/s]\n"
     ]
    }
   ],
   "source": [
    "PERSIST_DIR = 'storage'\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    os.makedirs(PERSIST_DIR)\n",
    "    # Connector. This is the SimpleDirectoryReader that reads the documents from the file system\n",
    "    documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "    # Verify that there are no empty documents\n",
    "    documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "    for doc in documents:\n",
    "        if not doc:\n",
    "            print(\"Documento vacío encontrado\")\n",
    "\n",
    "    # Index. This is the VectorStoreIndex that indexes the documents (Nodes in LlamaIndex)\n",
    "    # The input documents will be broken into nodes, and the embedding model will generate \n",
    "    # an embedding for each node.\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "    # # IN CASE YOU WANT TO USE A CUSTOM TEXT SPLITTER\n",
    "    # from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "    # text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "    # # IN CASE YOU WANT TO USE THE SAME TEXT SPLITTER FOR ALL INDEXES\n",
    "    # from llama_index.core import Settings\n",
    "\n",
    "    # Settings.text_splitter = text_splitter\n",
    "\n",
    "    # # IN CASE YOU WANT TO USE TEXT SPLITTER ONLY FOR THIS INDEX\n",
    "    # index = VectorStoreIndex.from_documents(\n",
    "    #     documents, transformations=[text_splitter]\n",
    "    # )\n",
    "\n",
    "    # Persist the index to disk (Storage)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # Rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    # Load index from storage\n",
    "    index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 424 0 (offset 0)\n",
      "Ignoring wrong pointing object 580 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 424 0 (offset 0)\n",
      "Ignoring wrong pointing object 580 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "VECTOR_STORE_DIR = 'chroma_db'\n",
    "if not os.path.exists(VECTOR_STORE_DIR):\n",
    "    os.makedirs(VECTOR_STORE_DIR)\n",
    "    # Load documents\n",
    "    documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "    # Verify that there are no empty documents\n",
    "    documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "    for doc in documents:\n",
    "        if not doc:\n",
    "            print(\"Documento vacío encontrado\")\n",
    "\n",
    "    # Initialize the ChromaDB client\n",
    "    db = chromadb.PersistentClient(path=VECTOR_STORE_DIR)\n",
    "\n",
    "    # Create a new collection\n",
    "    chroma_collection = db.get_or_create_collection('chroma_collection')\n",
    "\n",
    "    # Assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Create index\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "else:\n",
    "    # Initialize the ChromaDB client\n",
    "    db = chromadb.PersistentClient(path=VECTOR_STORE_DIR)\n",
    "\n",
    "    # Get the collection\n",
    "    chroma_collection = db.get_or_create_collection('chroma_collection')\n",
    "\n",
    "    # Assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Load index from storage\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store, storage_context=storage_context\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've already created an index, you can add new documents to your index using the insert method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex([])\n",
    "# for doc in documents:\n",
    "#     index.insert(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is to live in accordance with reason and the laws of nature, to maintain inner peace, to overcome pleasures and pains, to accept the transient nature of all things, and to face death with tranquility as a natural part of existence. Ultimately, the pursuit of philosophy is considered the key to salvation and understanding one's place in the universe.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='d32cb4a0-108e-4591-ba5f-9b6d2b282a22', embedding=None, metadata={'page_label': '13', 'file_name': 'Meditaciones-Marco-Aurelio.pdf', 'file_path': '/home/rprieto/RAG/Quick-Examples/../data/Meditaciones-Marco-Aurelio.pdf', 'file_type': 'application/pdf', 'file_size': 1163805, 'creation_date': '2025-03-20', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9415308d-cd17-464f-9369-45e95e8cb448', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'Meditaciones-Marco-Aurelio.pdf', 'file_path': '/home/rprieto/RAG/Quick-Examples/../data/Meditaciones-Marco-Aurelio.pdf', 'file_type': 'application/pdf', 'file_size': 1163805, 'creation_date': '2025-03-20', 'last_modified_date': '2025-03-20'}, hash='0aa2b78d6a1cb05812569d3d49dad9039819b32bc5644cf683b010a57f66857f')}, text='13 \\na un fin propuesto: y el fin de los seres racionales es obedecer a la razón y a la ley de la naturaleza, la más augusta de las ciudades y gobiernos. \\t17. El tiempo de la vida humana es un punto: la sustancia, fluente; la sensación, oscurecida; toda la constitución del cuerpo, corruptible; el alma, inquieta; el destino, enigmático; la fama, indefinible; en resumen, todas las cosas propias del cuerpo son a manera de un río; las del alma, sueño y vaho; la vida, una lucha, un destierro; la fama de la posteridad, olvido. ¿Qué hay, pues, que nos pueda llevar a salvamento? Una sola y única cosa: la filosofía. Y ésta consiste en conservar el dios interior sin ultraje ni daño, para que triunfe de placeres y dolores, para que no obre al acaso, y se mantenga lejos de toda falsedad y disimulo, al margen de que se haga o no se haga esto o aquello; además, para que acepte la parte que le tocare en los varios sucesos accidentales e integrantes de su parte, como procedentes de aquel origen de quien procede él mismo; y, en particular, para que aguarde la muerte en actitud plácida, no viendo en ella otra cosa más que la disolución de los elementos de que consta todo ser viviente. Si no hay nada temible para los mismos elementos en esta transformación incesante de uno en otro, ¿por qué temer la transformación y disolución de todas las otras cosas? Esto es conforme con la naturaleza: y nada es malo de cuanto a ella se acomoda. En Carnunto', mimetype='text/plain', start_char_idx=1, end_char_idx=1447, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7114528200836066)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 5\n",
    "response_mode = \"tree_summarize\" # Good for concise answers (summarization)\n",
    "\n",
    "query_engine =  index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    response_mode=response_mode,)\n",
    "response = query_engine.query(\"Cual es el significado de la vida?\")\n",
    "print(response.response)\n",
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available Chat Modes**\n",
    "- `best` - Turn the query engine into a tool, for use with a ReAct data agent or an OpenAI data agent, depending on what your LLM supports. OpenAI data agents require gpt-3.5-turbo or gpt-4 as they use the - function calling API from OpenAI.\n",
    "\n",
    "- `condense_question` - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine.\n",
    "\n",
    "- `context` - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "\n",
    "- `condense_plus_context` - A combination of condense_question and context. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "\n",
    "- `simple` - A simple chat with the LLM directly, no query engine involved.\n",
    "\n",
    "- `react` - Same as best, but forces a ReAct data agent.\n",
    "\n",
    "- `openai` - Same as best, but forces an OpenAI data agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parece que no has escrito nada en tu mensaje. Si tienes alguna pregunta o tema sobre el que te gustaría hablar, especialmente relacionado con la filosofía estoica o las \"Meditaciones\" de Marco Aurelio, no dudes en decírmelo. Estoy aquí para ayudarte.\n"
     ]
    }
   ],
   "source": [
    "# Then, at query time, the embedding model will be used again to embed the query text.\n",
    "system_prompt = \"Eres un maestro estoico capaz de aconsejar y hablar de esta filosofía tomando de referencia las meditaciones de Marco Aurelio\"\n",
    "chat_engine = index.as_chat_engine(chat_mode='context', verbose=False, system_prompt=system_prompt, similarity_top_k=5, llm=llm)\n",
    "for i in range(1):\n",
    "    response = chat_engine.chat(input())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hark! What dost thou seeketh, dear user? Speak thy mind, and I shall assist thee with words of Shakespearean flair.\n"
     ]
    }
   ],
   "source": [
    "# Predefined prompts example\n",
    "from llama_index.core.prompts.system import SHAKESPEARE_WRITING_ASSISTANT\n",
    "system_prompt=SHAKESPEARE_WRITING_ASSISTANT\n",
    "chat_engine = index.as_chat_engine(chat_mode='context', verbose=False, system_prompt=system_prompt, similarity_top_k=5)\n",
    "for i in range(1):\n",
    "    response = chat_engine.chat(input())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    \"\"\"Output containing the response, page numbers, and confidence.\"\"\"\n",
    "\n",
    "    response: str = Field(..., description=\"The answer to the question. It has to be maximum 10 words long.\")\n",
    "    example: list[str] = Field(..., description=\"3 literal full examples from the documents that supports the answer.\")\n",
    "    page_numbers: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"The page numbers of the sources used to answer this question. Do not include a page number if the context is irrelevant.\",\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        ...,\n",
    "        description=\"Confidence value between 0-1 of the correctness of the result.\",\n",
    "    )\n",
    "    confidence_explanation: str = Field(\n",
    "        ..., description=\"Explanation for the confidence score\"\n",
    "    )\n",
    "\n",
    "\n",
    "sllm = llm.as_structured_llm(output_cls=Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    llm=sllm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'La vida es una lucha y transformación constante.',\n",
       " 'example': ['la vida, una lucha, un destierro;',\n",
       "  'la vida de cada uno es como una exhalación',\n",
       "  'el mundo es una mutación continua: la vida, una imaginación.'],\n",
       " 'page_numbers': [13, 42, 21],\n",
       " 'confidence': 0.9,\n",
       " 'confidence_explanation': 'The answer is supported by multiple references discussing life as a struggle and transformation.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query(\"Que significado tiene la vida?\")\n",
    "response.response.model_dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist on Disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = 'storage'\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    os.makedirs(PERSIST_DIR)\n",
    "    # Connector. This is the SimpleDirectoryReader that reads the documents from the file system\n",
    "    documents = SimpleDirectoryReader(\"../Quick-Examples/data\").load_data()\n",
    "    # Verify that there are no empty documents\n",
    "    documents = SimpleDirectoryReader(\"../Quick-Examples/data\").load_data()\n",
    "    for doc in documents:\n",
    "        if not doc:\n",
    "            print(\"Documento vacío encontrado\")\n",
    "\n",
    "    # Index. This is the VectorStoreIndex that indexes the documents (Nodes in LlamaIndex)\n",
    "    # The input documents will be broken into nodes, and the embedding model will generate \n",
    "    # an embedding for each node.\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "    # # IN CASE YOU WANT TO USE A CUSTOM TEXT SPLITTER\n",
    "    # from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "    # text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "    # # IN CASE YOU WANT TO USE THE SAME TEXT SPLITTER FOR ALL INDEXES\n",
    "    # from llama_index.core import Settings\n",
    "\n",
    "    # Settings.text_splitter = text_splitter\n",
    "\n",
    "    # # IN CASE YOU WANT TO USE TEXT SPLITTER ONLY FOR THIS INDEX\n",
    "    # index = VectorStoreIndex.from_documents(\n",
    "    #     documents, transformations=[text_splitter]\n",
    "    # )\n",
    "\n",
    "    # Persist the index to disk (Storage)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # Rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    # Load index from storage\n",
    "    index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_STORE_DIR = 'chroma_db'\n",
    "if not os.path.exists(VECTOR_STORE_DIR):\n",
    "    os.makedirs(VECTOR_STORE_DIR)\n",
    "    # Load documents\n",
    "    documents = SimpleDirectoryReader(\"../Quick-Examples/data\").load_data()\n",
    "    # Verify that there are no empty documents\n",
    "    documents = SimpleDirectoryReader(\"../Quick-Examples/data\").load_data()\n",
    "    for doc in documents:\n",
    "        if not doc:\n",
    "            print(\"Documento vacío encontrado\")\n",
    "\n",
    "    # Initialize the ChromaDB client\n",
    "    db = chromadb.PersistentClient(path=VECTOR_STORE_DIR)\n",
    "\n",
    "    # Create a new collection\n",
    "    chroma_collection = db.get_or_create_collection('chroma_collection')\n",
    "\n",
    "    # Assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Create index\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "else:\n",
    "    # Initialize the ChromaDB client\n",
    "    db = chromadb.PersistentClient(path=VECTOR_STORE_DIR)\n",
    "\n",
    "    # Get the collection\n",
    "    chroma_collection = db.get_or_create_collection('chroma_collection')\n",
    "\n",
    "    # Assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Load index from storage\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store, storage_context=storage_context\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've already created an index, you can add new documents to your index using the insert method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex([])\n",
    "# for doc in documents:\n",
    "#     index.insert(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El significado de la vida se relaciona con la necesidad de obedecer a la razón y a la ley de la naturaleza, mantener la filosofía como guía para enfrentar placeres y dolores, aceptar la transformación constante de todas las cosas y aguardar la muerte con serenidad al comprender que es parte de la disolución natural de los elementos que componen todo ser viviente.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='3d4d5a74-c933-4c7b-8991-450264d52cf2', embedding=None, metadata={'page_label': '13', 'file_name': 'Meditaciones-Marco-Aurelio.pdf', 'file_path': '/home/rprieto/RAG/Quick-Examples/../Quick-Examples/data/Meditaciones-Marco-Aurelio.pdf', 'file_type': 'application/pdf', 'file_size': 1163805, 'creation_date': '2025-03-20', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e6bd5f18-0e02-4ae4-913f-a091ecd6f997', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'Meditaciones-Marco-Aurelio.pdf', 'file_path': '/home/rprieto/RAG/Quick-Examples/../Quick-Examples/data/Meditaciones-Marco-Aurelio.pdf', 'file_type': 'application/pdf', 'file_size': 1163805, 'creation_date': '2025-03-20', 'last_modified_date': '2025-03-20'}, hash='499a005804dcf5ab895ed5a8626a5d79e7dbcf792e737973cf5acc13138f31e2')}, text='13 \\na un fin propuesto: y el fin de los seres racionales es obedecer a la razón y a la ley de la naturaleza, la más augusta de las ciudades y gobiernos. \\t17. El tiempo de la vida humana es un punto: la sustancia, fluente; la sensación, oscurecida; toda la constitución del cuerpo, corruptible; el alma, inquieta; el destino, enigmático; la fama, indefinible; en resumen, todas las cosas propias del cuerpo son a manera de un río; las del alma, sueño y vaho; la vida, una lucha, un destierro; la fama de la posteridad, olvido. ¿Qué hay, pues, que nos pueda llevar a salvamento? Una sola y única cosa: la filosofía. Y ésta consiste en conservar el dios interior sin ultraje ni daño, para que triunfe de placeres y dolores, para que no obre al acaso, y se mantenga lejos de toda falsedad y disimulo, al margen de que se haga o no se haga esto o aquello; además, para que acepte la parte que le tocare en los varios sucesos accidentales e integrantes de su parte, como procedentes de aquel origen de quien procede él mismo; y, en particular, para que aguarde la muerte en actitud plácida, no viendo en ella otra cosa más que la disolución de los elementos de que consta todo ser viviente. Si no hay nada temible para los mismos elementos en esta transformación incesante de uno en otro, ¿por qué temer la transformación y disolución de todas las otras cosas? Esto es conforme con la naturaleza: y nada es malo de cuanto a ella se acomoda. En Carnunto', mimetype='text/plain', start_char_idx=1, end_char_idx=1447, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7135531613227234)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 5\n",
    "response_mode = \"tree_summarize\" # Good for concise answers (summarization)\n",
    "\n",
    "query_engine =  index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    response_mode=response_mode,)\n",
    "response = query_engine.query(\"Cual es el significado de la vida?\")\n",
    "print(response.response)\n",
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available Chat Modes**\n",
    "- `best` - Turn the query engine into a tool, for use with a ReAct data agent or an OpenAI data agent, depending on what your LLM supports. OpenAI data agents require gpt-3.5-turbo or gpt-4 as they use the - function calling API from OpenAI.\n",
    "\n",
    "- `condense_question` - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine.\n",
    "\n",
    "- `context` - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "\n",
    "- `condense_plus_context` - A combination of condense_question and context. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "\n",
    "- `simple` - A simple chat with the LLM directly, no query engine involved.\n",
    "\n",
    "- `react` - Same as best, but forces a ReAct data agent.\n",
    "\n",
    "- `openai` - Same as best, but forces an OpenAI data agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rprieto/miniconda3/envs/RAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm_openai = OpenAI(model=\"gpt-4o-mini\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm_gemini = Gemini(model=\"models/gemini-2.0-flash\", api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtud.\n"
     ]
    }
   ],
   "source": [
    "# Then, at query time, the embedding model will be used again to embed the query text.\n",
    "system_prompt = \"Eres un maestro estoico capaz de aconsejar y hablar de esta filosofía tomando de referencia las meditaciones de Marco Aurelio\"\n",
    "chat_engine = index.as_chat_engine(chat_mode='context', verbose=False, system_prompt=system_prompt, similarity_top_k=5, llm=llm_openai)\n",
    "for i in range(1):\n",
    "    response = chat_engine.chat(input())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amor.\n"
     ]
    }
   ],
   "source": [
    "# Predefined prompts example\n",
    "from llama_index.core.prompts.system import SHAKESPEARE_WRITING_ASSISTANT\n",
    "system_prompt=SHAKESPEARE_WRITING_ASSISTANT\n",
    "chat_engine = index.as_chat_engine(chat_mode='context', verbose=False, system_prompt=system_prompt, similarity_top_k=5, llm=llm_openai)\n",
    "for i in range(1):\n",
    "    response = chat_engine.chat(input())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    \"\"\"Output containing the response, page numbers, and confidence.\"\"\"\n",
    "\n",
    "    response: str = Field(..., description=\"The answer to the question. It has to be maximum 10 words long.\")\n",
    "    example: list[str] = Field(..., description=\"3 literal full examples from the documents that supports the answer.\")\n",
    "    page_numbers: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"The page numbers of the sources used to answer this question. Do not include a page number if the context is irrelevant.\",\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        ...,\n",
    "        description=\"Confidence value between 0-1 of the correctness of the result.\",\n",
    "    )\n",
    "    confidence_explanation: str = Field(\n",
    "        ..., description=\"Explanation for the confidence score\"\n",
    "    )\n",
    "\n",
    "sllm_openai = llm_openai.as_structured_llm(output_cls=Output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=50,\n",
    "    llm=sllm_openai,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'Obrar conforme a la naturaleza y la justicia.',\n",
       " 'example': ['\"El fin de los seres racionales es obedecer a la razón y a la ley de la naturaleza.\"',\n",
       "  '\"Conviene aprovechar el presente, usándolo con reflexión y justicia.\"',\n",
       "  '\"La misión de la naturaleza universal se reduce a trasladar allí lo que estaba ahí, a transformarlo.\"'],\n",
       " 'page_numbers': [13, 19, 51],\n",
       " 'confidence': 0.9,\n",
       " 'confidence_explanation': 'The answer is supported by multiple references discussing the purpose of life in terms of nature and justice.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query(\"Cuál es el sentido de la vida?\")\n",
    "response.response.model_dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
